{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writeup\n",
    "\n",
    "---\n",
    "\n",
    "**Advanced Lane Finding Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./camera_cal_output/calibration2.jpg \"Calibration.\"\n",
    "[image2.1]: ./test_images/test1.jpg \"Original Image\"\n",
    "[image2.2]: ./output_images/test1_undist.jpg \"Undistorted Image\"\n",
    "[image3]: ./output_images/test1_combo.jpg \"Binary Example\"\n",
    "[image4]: ./output_images/test1_warp.jpg \"Warp Example\"\n",
    "[image5]: ./output_images/test1_lane.jpg \"Fit Visual\"\n",
    "[image6]: ./output_images/test1.jpg \"Output\"\n",
    "[video1]: ./project_video.mp4 \"Video\"\n",
    "\n",
    "## [Rubric](https://review.udacity.com/#!/rubrics/571/view) Points\n",
    "\n",
    "### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  \n",
    "\n",
    "---\n",
    "\n",
    "### Camera Calibration\n",
    "\n",
    "#### 1. Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image.\n",
    "\n",
    "The code for this step is contained in the first code cell of the IPython notebook located in \"./CameraCalibration.ipynb\".  \n",
    "\n",
    "I start by preparing \"object points\", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image.  Thus, `objp` is just a replicated array of coordinates, and `objpoints` will be appended with a copy of it every time I successfully detect all chessboard corners in a test image.  `imgpoints` will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.  \n",
    "\n",
    "I then used the output `objpoints` and `imgpoints` to compute the camera calibration and distortion coefficients using the `cv2.calibrateCamera()` function.  I applied this distortion correction to the test image using the `cv2.undistort()` function and obtained this result: \n",
    "\n",
    "![alt text][image1]\n",
    "\n",
    "### Pipeline (single images)\n",
    "\n",
    "#### 1. Provide an example of a distortion-corrected image.\n",
    "\n",
    "I called to `cv2.undistort()` to undistort the image by using mtx, dist created in calibration.<p>\n",
    "The original Image:\n",
    "![alt text][image2.1]\n",
    "The undistorted Image:\n",
    "![alt_text][image2.2]\n",
    "\n",
    "#### 2. Color transforms, gradients or other methods to create a thresholded binary image. \n",
    "\n",
    "I used a combination of color and gradient thresholds to generate a binary image (3rd code cell of the IPython notebook \"./AdvancedLaneFindingImage.ipynb\").  Here's an example of my output for this step.\n",
    "\n",
    "![alt text][image3]\n",
    "\n",
    "#### 3. Perspective transform and provide an example of a transformed image.\n",
    "\n",
    "The code for my perspective transform includes a function called `transform_perspective()`, which appears in 4th code cell of the IPython notebook (\"./AdvancedLaneFindingImage.ipynb\").  The `transform_perspective()` function takes as inputs an image (`img`)..  I chose the hardcode the source and destination points in the following manner:\n",
    "\n",
    "```python\n",
    "src = np.float32(\n",
    "    [[(img_size[0] / 2) - 55, img_size[1] / 2 + 100],\n",
    "    [((img_size[0] / 6) - 10), img_size[1]],\n",
    "    [(img_size[0] * 5 / 6) + 60, img_size[1]],\n",
    "    [(img_size[0] / 2 + 55), img_size[1] / 2 + 100]])\n",
    "dst = np.float32(\n",
    "    [[(img_size[0] / 4), 0],\n",
    "    [(img_size[0] / 4), img_size[1]],\n",
    "    [(img_size[0] * 3 / 4), img_size[1]],\n",
    "    [(img_size[0] * 3 / 4), 0]])\n",
    "```\n",
    "\n",
    "This resulted in the following source and destination points:\n",
    "\n",
    "| Source        | Destination   | \n",
    "|:-------------:|:-------------:| \n",
    "| 585, 460      | 320, 0        | \n",
    "| 203, 720      | 320, 720      |\n",
    "| 1127, 720     | 960, 720      |\n",
    "| 695, 460      | 960, 0        |\n",
    "\n",
    "I verified that my perspective transform was working as expected by drawing the `src` and `dst` points onto a test image and its warped counterpart to verify that the lines appear parallel in the warped image.\n",
    "\n",
    "![alt text][image4]\n",
    "\n",
    "#### 4. Identified lane-line pixels and fit their positions with a polynomial?\n",
    "\n",
    "The code for my identifing lane-line includes a function called `find_lane_pixels()`, `fit_polynomial()`, which appears in 6th code cell of the IPython notebook (\"./AdvancedLaneFindingImage.ipynb\").  Then I fit my lane lines with a 2nd order polynomial kinda like this:\n",
    "\n",
    "![alt text][image5]\n",
    "\n",
    "#### 5. Calculated the radius of curvature of the lane and the position of the vehicle with respect to center.\n",
    "\n",
    "I did this in `measure_curvature_real()`, which appears in 7th code cell of the IPython notebook (\"./AdvancedLaneFindingImage.ipynb\").\n",
    "\n",
    "#### 6. Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.\n",
    "\n",
    "I implemented in `warp_back()`, which appears in 8th code cell of the IPython notebook (\"./AdvancedLaneFindingImage.ipynb\").  Here is an example of my result on a test image:\n",
    "\n",
    "![alt text][image6]\n",
    "\n",
    "---\n",
    "\n",
    "### Pipeline (video)\n",
    "\n",
    "#### 1.Final video output.\n",
    "I changed a little bit for video, because we can use prior position to find lanes.  The code is in IPython notebook (\"./AdvancedLaneFindingVideo.ipynb\").\n",
    "Here's a [link to my video result](./project_video_out.mp4)\n",
    "\n",
    "---\n",
    "\n",
    "### Discussion\n",
    "\n",
    "#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?\n",
    "\n",
    "I tested challenge_video.mp4 and harder_challenge_video.mp4, but they are litterally hard to identify lanes.  And it is hard to find where the problem is , because they are streaming. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
